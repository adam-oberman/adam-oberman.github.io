# MATH/COMP 562: Theory of Machine Learning
### Winter 2021, Adam Oberman https://www.adamoberman.net/
### COVID
McGill has announced online teaching for this course, at least until January 24th.  I hope we can return to in person teaching, which will be better for group work and presentations. 
### Prerequisites
Prerequisites: MATH 462 or COMP 451 or (COMP 551, MATH 222, MATH 223 and MATH 324) or ECSE 551.
### Teaching Assistants
Viet Nguyen: baviet.nguyen@mail.mcgill.ca
Gabriela Moisescu-Pareja: gabriela.moisescu-pareja@mail.mcgill.ca
### Related Courses
- [Math 462, Fall 2021](https://github.com/adam-oberman/adam-oberman.github.io/tree/main/Lectures)
  - To make sure that we all have the same background, there will be some overlap with this course in the first few weeks of term (of the 80+ students in 562, under 20 took 452).   Students from 462 can start their projet, and be excused from the small number of HW problems which overlap.
- [COMP 551 Applied Machine Learning](https://www.siamak.page/courses/COMP551F21/index.html)
  - This course focuses on implementation, rather than theory.  This a complementary course.
### References
- [Notes from Math 462](https://github.com/adam-oberman/adam-oberman.github.io/tree/main/Lectures)
- NLP Book https://mitpress.mit.edu/books/introduction-natural-language-processing
- Deep Learning book  https://www.deeplearningbook.org/
- RL Book http://www.incompleteideas.net/book/the-book-2nd.html
#### Additional general references
- [Mathematics for Machine Learning by Diesenroth](https://mml-book.github.io/) Review of prerequisites, with a ML focus.
- [Probabilistic Machine Learning: An Introduction by Kevin Patrick Murphy](https://probml.github.io/pml-book/book1.html) Good reference for many topics, but not presented as digestible lectures
#### ML Theory references
- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) by Shalev-Shwartz and Ben David  This book is very good for presenting machine learning problems.
- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/) by Mohri, Rostamizadeh, Talwalkar. Rigorous proofs of generalization bounds for classification problems.  VC dimension, Rademacher complexity. 
- [High dimensional statistics, a non-asymptotic viewpoint](https://people.eecs.berkeley.edu/~wainwrig/) by Martin J Wainwright. (First couple chapters only),  good math reference for concentration of measure which is used in the generalization bounds.  

### Grading (TBD)
I'm hoping we will return to in person teaching. 
Graded material will depend on if we return to in person classes.
The grading scheme will be adjusted once we know if we can return to in person teaching.  
- Attendance and Participation  (attend group presentations, ask questions, screen on for zoom when possible): 10%
- Group Project.  Writeup / presentation (15/15): 30% 
- Homework Quizzes / Homework assignments  (in person 10/20, online 0/30): 30%
- Final exam / Final Report (in person 30/0, online 0/30): 30%
*Changes due to COVID* I reserve the right to make changes to the assignment structure/grading scheme in response to the COVID situation.  However, significant changes will be presented to the class for feedback, and will go forward only if there is strong agreement by the class. 

### Key Times and Dates (TBD)
Refer to [McGill key Dates](https://www.mcgill.ca/importantdates/key-dates#Winter_2022)
- Classes are every Tuesday and Thursday 11:30-1pm.  
 - First class: Thursday Jan 6th. (11:30am-1pm, zoom link on mycourses page).   
 - Reading Break: Feb 28 - March 4th
- Classes end Tuesday April 12th. 
- Midterm dates: TBD

### Course Topics
- Prerequisite material (which will be covered in more detail later in the course) 
  - Foundational Machine Learning: Classification, binary and multi-classes.  Classification losses, convex surrogate losses.  
  - Scoring function and class probability interpretations.  
  - Training models: Optimization, stochastic gradient descent. 
### Schedule
- Week 1: Introduction to Machine Learning, Deep Learning.  Topics in deep learning: Reinforcement Learning, Natural Language Processing, Computer Vision.   [Notes from Lecture 1](https://github.com/adam-oberman/adam-oberman.github.io/files/7824026/Lecture.1.2022.01.06.MC.562.pdf)
- Weeks 2, 3, 4: Generalization in (shallow) machine learning.  References: Shalev-Shwartz (first part), Mohri (latter part). 
  - PAC Learning bounds, VC dimension, Concentration of measure, Rademacher complexity.
- Later in the course: Introduction to deep learning. Introduction and math background for computer vision/generative models, NLP, RL.
