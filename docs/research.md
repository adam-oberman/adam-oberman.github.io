---
layout: default
title: ML/AI Research
---

# Machine Learning/AI Research 
- Generative models: Normalizing flows, diffusion models
- Architectures: EuclidNets
- Calibration
- ML for PDEs / learning PDE solvers
- adversarial robustness machine learning
- optimization of deep neural networks
  
see also [Applied Mathematics Research](research_older.md)


# Publications 
**2025**
- *Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs* [arxiv](https://arxiv.org/abs/2505.12049) **NeurIPS 2025 Spotlight** Mehran Shakerinava, Siamak Ravanbakhsh, Adam Oberman
- *Numerical PDE solvers outperform neural PDE solvers* [arxiv](https://arxiv.org/abs/2507.21269) Patrick Chatain, Michael Rizvi‑Martel, Guillaume Rabusseau and Adam Oberman 
- *Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?* [arxiv](https://arxiv.org/abs/2502.15657) Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King


**2024**
- Harnessing small projectors and multiple views for efficient vision pretraining [NeurIPS 2024 poster](https://neurips.cc/virtual/2024/poster/94719) Arna Ghosh, Kumar Krishna Agrawal, Shagun Sodhani, Adam Oberman, Blake Richards  
- Multi-Resolution Continuous Normalizing Flows Vikram Voleti, Chris Finlay, Adam Oberman, Christopher Pal, Annals of Mathematics and Artificial Intelligence 2024, [journal](https://doi.org/10.1007/s10472-024-09939-5) [arxiv](https://arxiv.org/abs/2106.08462)
- Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity David Williams-King, Linh Le, Adam Oberman, Yoshua Bengio [Safe Generative AI Workshop @ NeurIPS 2024](https://safegenaiworkshop.github.io/) [open review](https://openreview.net/forum?id=QO7dF42YEb)

**2023**
-  Deep PDE solvers for Subgrid Modelling and Out-of-Distribution Generalization, Patrick Chatain, Adam Oberman [NeurIPS 2023 workshop](https://neurips.cc/virtual/2023/workshop/66538) [paper](https://neurips.cc/virtual/2023/75472)
-   EuclidNets: An Alternative Operation for Efficient Inference of Deep Learning  Models Xinlin Li and Mariana Prazeres and Adam M. Oberman and Alireza  Ghaffari and Masoud Asgharian and Vahid Partovi Nia, [SN Computer Science 2023](https://doi.org/10.1007/s42979-023-01921-y) [arXiv](https://arxiv.org/abs/2212.11803)

**2022**

1. On the Generalization of Representations in Reinforcement Learning.  Charline Le Lan, Stephen Tu, Adam Oberman, Rishabh Agarwal, Marc G.  Bellemare: [AISTATS 2022: 4132-4157](https://aistats.org/aistats2022/accepted.html) [arxiv](https://arxiv.org/abs/2203.00543)
2. FairCal: Fairness Calibration for Face Verification. Tiago Salvador, Stephanie  Cairns, Vikram Voleti, Noah Marshall, Adam M. Oberman. [ICLR 2022](https://openreview.net/forum?id=nRj0NcmSuxb)  [arxiv](https://arxiv.org/abs/2106.03761)
3. EuclidNets: Combining Hardware and Architecture Design for Efficient Training and  Inference. Mariana Oliveira Prazeres, Xinlin Li, Adam M. Oberman, Vahid  Partovi Nia. ICPRAM 2022: 141-151 https://arxiv.org/abs/2212.11803
4. A Reproducible and Realistic Evaluation of Partial Domain Adaptation  Methods. Tiago Salvador, Kilian Fatras, Ioannis Mitliagkas, and Adam  Oberman. NeurIPS Distribution Shifts Workshop, 2022 https://arxiv.org/abs/2210.01210
5. ImageNet-Cartoon and ImageNet-Drawing: two domain shift datasets for ImageNet. Tiago  Salvador and Adam M. Oberman. ICML Shift Happens Workshop, 2022
6. Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models. Vikram  Voleti, Christopher Pal, Adam Oberman. NeurIPS 2022 Workshop on  Score-Based Methods https://arxiv.org/abs/2210.12254
7. Improving Continuous Normalizing Flows using a Multi-Resolution Framework.  Voleti, Vikram, Chris Finlay, Adam M. Oberman, and Christopher Pal. In  ICML Workshop on Invertible Neural Networks, Normalizing Flows, and  Explicit Likelihood Models.

**2021**

1. Stochastic Gradient Descent with Polyak's Learning Rate Mariana Prazeres, Adam Oberman *Journal of Scientific Computing* 89.1 (2021): 1-16. [arxiv](https://arxiv.org/abs/1903.08688)
2. Learning normalizing flows from Entropy-Kantorovich potentials Chris Finlay, Augusto Gerolin, Adam M Oberman, Aram-Alexandre Pooladian [arxiv](https://arxiv.org/abs/2006.06033) [INNF+ 2021 workshop](https://invertibleworkshop.github.io/accepted_papers/index.html) (2021)
3. (Journal) Scaleable input gradient regularization for adversarial robustness Chris Finlay, Adam Oberman, Machine Learning with Applications (MLWA), 3 2021 [arxiv](https://arxiv.org/abs/1905.11468) [github](https://github.com/cfinlay/tulip)
4. "Methods and systems for computing an output of a neural network layer". US Patent application 17/317,833 (filed May 11, 2021)

**2020**

1. (Journal and Conference) *Nesterov's method with decreasing learning rate leads to accelerated stochastic gradient descent*, Maxime Laborde, Adam Oberman [arxiv](https://arxiv.org/abs/1908.07861) *(Older version: A Lyapunov analysis for accelerated gradient method: from deterministic to stochastic case*, Maxime Laborde, Adam Oberman [AISTATS 2020](https://www.aistats.org/accepted.html) (Proceedings of Machine Learning Research)
2. (Conference) *Calibrated Top-1 Uncertainty Estimates for classification by score based models* Adam Oberman Chris Finlay, Alexander Iannantuono, Tiago Salvador [arxiv](https://arxiv.org/abs/1903.09215) spotlight presentation at Spotlight Talk at [UDL ICML Workshop](https://icml.cc/virtual/2020/workshop/5717) (140 accepted papers, 6 contributed talks, 8 spotlight talks).
3. (Conference) *How to train your neural ODE: the world of Jacobian and kinetic regularization* Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, Adam Oberman [arxiv ](https://arxiv.org/abs/2002.02798) [ICML 2020](https://proceedings.icml.cc/paper/2020/hash/98c56bce74669e2e4e7a9fc1caa8c326)
4. (Conference) *A principled approach for generating adversarial images under non-smooth dissimilarity metrics* Aram-Alexandre Pooladian, Chris Finlay, Tim Hoheisel, Adam Oberman [AISTATS 2020](https://www.aistats.org/accepted.html) (Proceedings of Machine Learning Research) [arxiv](https://arxiv.org/abs/1908.01667)
5. (Journal and Conference) *No collision transportation maps,* Levon Nurbekyan, Alexander Iannantuono, Adam M. Oberman [NeurIPS 2019 OTML workshop](https://sites.google.com/view/otml2019/schedule) [Journal of Scientific Computing](https://doi.org/10.1007/s10915-020-01143-x) 2020 [arxiv](https://arxiv.org/abs/1912.02317)
6. (Journal) *A regularization interpretation of the proximal point method for weakly convex functions* Tim Hoheisel, Maxime Laborde, Adam Oberman  [Journal of Dynamics and Games ](http://dx.doi.org/10.3934/jdg.2020005)2020
7. (Conference) *Partial differential equation regularization for supervised machine learning,* Adam Oberman 75 Years of Mathematicas of Computation Symposium (2020) [AMS Contemporary Math](https://www.ams.org/books/conm/) [arxiv](http://arxiv.org/abs/1910.01612)

**2019, 2018**

1. (Conference) *The LogBarrier adversarial attack: making effective use of decision boundary information* Chris Finlay, Aram-Alexander Pooladian, Adam Oberman [ICCV](http://openaccess.thecvf.com/content_ICCV_2019/html/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.html) 2019 [arxiv](https://arxiv.org/abs/1903.10396)
2. *Parle: parallelizing stochastic gradient descent* Pratik Chaudhari, Carlo Baldassi, Riccardo Zecchina, Stefano Soatto, Ameet Talwalkar, Adam Oberman [SysML](http://www.sysml.cc/)  2018 [arxiv](https://arxiv.org/abs/1707.00424)
3. *Stochastic Backward Euler: An Implicit Gradient Descent Algorithm for k-means Clustering* Penghang Yin, Minh Pham, Adam Oberman, Stanley Osher; [Journal of Scientific Computing](https://doi.org/10.1007/s10915-018-0744-4) 2018 [arxiv](https://arxiv.org/abs/1710.07746)
4. *Deep Relaxation: partial differential equations for optimizing deep neural networks* Pratik Chaudhari, Adam M. Oberman, Stanley Osher, Stefano Soatto, Guillame Carlier; [Research in Math Sciences](https://link.springer.com/article/10.1007/s40687-018-0148-y) 2018 [arxiv](https://arxiv.org/abs/1704.04932)
5. *Approximate Homogenization of fully nonlinear elliptic PDEs* Chris Finlay and Adam M. Oberman; [Journal of Scientific Computing](https://link.springer.com/journal/10915) 2018, [arxiv ](https://arxiv.org/abs/1710.10311)
6. *Approximate Homogenization of convex nonlinear elliptic PDEs* Chris Finlay and Adam M. Oberman; [Comm Math Sci](http://intlpress.com/site/pub/pages/journals/items/cms/_home/_main/) 2018, [arxiv ](https://arxiv.org/abs/1710.10309)



# Presentations
- Accelerated SGD [slides](https://www.adamoberman.net/uploads/6/2/4/2/62426505/accelerate_sgd_new_slides_2020_05.pdf)
- Mathematics applied to Deep Neural Networks (Nov 2018): [slides](https://www.adamoberman.net/uploads/6/2/4/2/62426505/math_comp_brown_11_02.pdf), [Video](https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=ae9989ff-5e3e-4f28-9332-a98d012a4614)
- Presentation on Generalization of Deep Neural Networks and Adversarial Robustness (Oct 2018) [slides](https://www.adamoberman.net/uploads/6/2/4/2/62426505/miml_oct_30.pdf)
- Introductory (non research) [talk for undergraduates](https://www.adamoberman.net/uploads/6/2/4/2/62426505/seminar_1_introduction.pdf) on Math and Deep Neural Networks (Sept 2018), based on the [JASON report](https://fas.org/irp/agency/dod/jason/)
- Deep Relaxation: [slides.pdf](https://www.adamoberman.net/uploads/6/2/4/2/62426505/2017_08_30_ipam.pdf) and [Video of Lecture](http://www.ipam.ucla.edu/programs/workshops/mean-field-games/?tab=schedule) at 2017 09 IPAM Conference on Mean Field Games

