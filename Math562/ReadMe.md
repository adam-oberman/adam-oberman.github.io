# MATH/COMP 562: Theory of Machine Learning, Winter 2023

Instructor: Adam Oberman 
Class: Tues/Thurs 2:30-4pm, LEA 14

### Prerequisites
Prerequisites: MATH 462 or COMP 451 or (COMP 551, MATH 222, MATH 223 and MATH 324) or ECSE 551.

### Related Courses
- Math 462:  This is a prequel to MATH 562, recommended for math students. 
- COMP 551 Applied Machine Learning.  This is a graduate course, focused on implementation, rather than theory.  This a complementary course.

### References
-  Book draft by [Francis Bach](https://www.di.ens.fr/~fbach/) https://www.di.ens.fr/%7Efbach/ltfp_book.pdf
- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) by Shalev-Shwartz and Ben David  This book is very good for presenting machine learning problems.
- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/) by Mohri, Rostamizadeh, Talwalkar. Rigorous proofs of generalization bounds for classification problems.  VC dimension, Rademacher complexity. 
- [High dimensional statistics, a non-asymptotic viewpoint](https://people.eecs.berkeley.edu/~wainwrig/) by Martin J Wainwright. (First couple chapters only),  good math reference for concentration of measure which is used in the generalization bounds.  

### Grading 
- Attendance and Participation  (attend group presentations, ask questions, screen on for zoom when possible): 10%
- Group Project.  Writeup and presentation:30% 
- Homework: 30%
- Individual Project/Final Report: 30%

### Key Times and Dates (TBD)
Refer to [McGill key Dates](https://www.mcgill.ca/importantdates/key-dates#Winter_2023)

### Course Topics

  - Foundational Machine Learning: Classification, binary and multi-classes.  Classification losses, convex surrogate losses.  
  - Scoring function and class probability interpretations.  
  - Training models: Optimization, stochastic gradient descent. 
  - Later in the course: Introduction to deep learning. Introduction and math background for computer vision/generative models, NLP, RL.
  - Part 1: Introduction to Machine Learning, Deep Learning.  Topics in deep learning: Reinforcement Learning, Natural Language Processing, Computer Vision.   Generalization in (shallow) machine learning.  References: Shalev-Shwartz (first part), Mohri (latter part).  PAC Learning bounds, VC dimension, Concentration of measure, Rademacher complexity.

