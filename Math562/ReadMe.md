# MATH/COMP 562: Theory of Machine Learning, Winter 2023

Instructor: Adam Oberman 
Class: Tues/Thurs 2:30-4pm, LEA 14

### Prerequisites
Prerequisites: MATH 462 or COMP 451 or (COMP 551, MATH 222, MATH 223 and MATH 324) or ECSE 551.

### Related Courses
- Math 462:  This is a prequel to MATH 562, recommended for math students. 
- COMP 551 Applied Machine Learning.  This is a graduate course, focused on implementation, rather than theory.  This a complementary course.

### References
-  Book draft by [Francis Bach](https://www.di.ens.fr/~fbach/) https://www.di.ens.fr/%7Efbach/ltfp_book.pdf
- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) by Shalev-Shwartz and Ben David  This book is very good for presenting machine learning problems.
- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/) by Mohri, Rostamizadeh, Talwalkar. Rigorous proofs of generalization bounds for classification problems.  VC dimension, Rademacher complexity. 
- [High dimensional statistics, a non-asymptotic viewpoint](https://people.eecs.berkeley.edu/~wainwrig/) by Martin J Wainwright. (First couple chapters only),  good math reference for concentration of measure which is used in the generalization bounds.  

### Grading 
- Attendance and Participation  (attend group presentations, ask questions, screen on for zoom when possible): 10%
- Group Project.  Writeup and presentation:30% 
- Homework: 30%
- Individual Project/Final Report: 30%

### Key Times and Dates (TBD)
Refer to [McGill key Dates](https://www.mcgill.ca/importantdates/key-dates#Winter_2023)

### Course Topics

- Prerequisite material (which will be covered in more detail later in the course) 
  - Foundational Machine Learning: Classification, binary and multi-classes.  Classification losses, convex surrogate losses.  
  - Scoring function and class probability interpretations.  
  - Training models: Optimization, stochastic gradient descent. 
  - Later in the course: Introduction to deep learning. Introduction and math background for computer vision/generative models, NLP, RL.

### Outline

- Part 1: Introduction to Machine Learning, Deep Learning.  Topics in deep learning: Reinforcement Learning, Natural Language Processing, Computer Vision.   Generalization in (shallow) machine learning.  References: Shalev-Shwartz (first part), Mohri (latter part).  PAC Learning bounds, VC dimension, Concentration of measure, Rademacher complexity.

### Homework

- [Math562_HW1_V2.pdf](https://github.com/adam-oberman/adam-oberman.github.io/files/7945227/Math562_HW1_V2.pdf)

### Group and individual projects

- Stage 1,Tues Feb 1st (choose topics and groups).  See suggestions [Suggested Paper Links.pdf](https://github.com/adam-oberman/adam-oberman.github.io/files/7951096/Suggested.Paper.Links.pdf)
- Stage 2 (outline and plan) Thurs Feb 3rd.  
- Stage 3: see instructions [MathComp 562 Project Description.pdf](https://github.com/adam-oberman/adam-oberman.github.io/files/7943833/MathComp.562.Project.Description.pdf)

### Course Notes

Chapter References [Machine Learning]: 

- [SS] "[Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/)" by Shalev-Shwartz and Ben David.  
- [M] "Foundations of Machine Learning" Mohri, Rostamizadeh, Talwalkar. 
  Chapter References [Reinforcement Learning]:
- "[Reinforcement Learning: Theory and Algorithms](https://rltheorybook.github.io)" by Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun.
- "[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)" by Csaba Szepesvari

