# Lecture Notes :  MATH 462, Fall 2021

## Course Notes (same material, order may differ)
- [Course Notes](https://github.com/adam-oberman/adam-oberman.github.io/blob/main/Lectures/Math462_Lecture_Notes.pdf). These notes will be revised, so the material may change slightly from week to week.  I will post a final version after each section. 


## Handwritten notes from class  
- [Lecture 1](09%2001%20Lecture%201.pdf)
- [Lecture 2](09%2003%20Lecture%202.pdf)
- [Lecture 3](09%2008%20Lecture%203.pdf)
- [Lecture 4](09%2010%20Lecture%204.pdf)

## Zoom recordings 
- (No recording for lectures 1 and 2) 
- [Lecture 3](https://mcgill.zoom.us/rec/share/VKdYKjgxXbdlP9_8l3xcSKz7E2A7Z_gwyOpYjbO1n9XQ-gSIO51ITa9Ug83cjejV.ZFHqMEOCdcJpXMx0?startTime=1631109875000)

## By lecture
### Week 1, lecture 1, Sept 1 (W)
- Introduction: example problems and datasets, meet and greet.
- Reference: Murphy Ch1, [Lecture 1](09%2001%20Lecture%201.pdf)
### Week 1, Lecture 2, Sept 3 (F)
- Set up the supervised learning problem for regression, [Lecture 2](09%2001%20Lecture%202.pdf)
### Week 2, Lecture 3, Sept 8 (W)
- Regression, other losses, compare losses.
- Calculus to find the minimizer of the (ELM) problem. 
- Reference: Course notes. [Lecture 3](09%2008%20Lecture%203.pdf)
### Week 2, Lecture 4, Sept 10 (F)
- Review of calculus and vector calculus, chain rule, gradient
- Compute gradient of the (EL) expected loss function
- Quadratic regression case 
- References: [Lecture 4](09%2008%20Lecture%203.pdf), Course Notes, Chapter "Gradients and Minimizers"

