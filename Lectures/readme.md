# Lecture Notes :  MATH 462, Fall 2021

## Course Notes (typeset)
- [Final version of course notes Part 1](Math462_Lecture_Notes_Part_1.pdf)
- [Final version of course notes (For HW 2) Part 2](https://www.overleaf.com/read/kwvkfdtnwtzp)
- [Updated version of course note Part 2 (not for HW 2)](https://www.overleaf.com/read/yzqkgwpswjkd)
- [Course notes part 3 (working version)](https://www.overleaf.com/read/syvzdvznfmby)

### Homework
- Homework 1 due noon, Monday Sept 20th [Homework 1 Revised](Math462_HW1_V4.pdf)
- [Homework 1 Solutions](https://github.com/adam-oberman/adam-oberman.github.io/blob/main/Lectures/MATH462.HW1.Soln.pdf)
- Homework 2, due 5pm, Tuesday Oct 5th, [Homework 2](Math462_HW2.pdf)
- Homework 3, due 5pm, Tuesday Oct 19th, [Homework 3](https://www.overleaf.com/read/pmbgwhxztmxt)


## Handwritten in class notes 
- [Lecture 1](09%2001%20Lecture%201.pdf)
- [Lecture 2](09%2003%20Lecture%202.pdf)
- [Lecture 3](09%2008%20Lecture%203.pdf)
- [Lecture 4](09%2010%20Lecture%204.pdf)
- [Tutorial 1](Tutorial%201_%20Linear%20Regression%20And%20Matrices.pdf)
- [Lecture 5](09%2015%20Math%20462%20Lecture%205.pdf)
- [Lecture 6](09.17.Math462.Lecture6%20.pdf)
- [Lecture 7](09.17.Math462.Lecture6%20.pdf)
- [Lecture 8](https://github.com/adam-oberman/adam-oberman.github.io/blob/main/Lectures/Math462.09.24.F.L8.pdf)

## Lectures Part 3
### Week 6, Lectures
- [Lecture 13](20.10.2021.Lecture13.pdf)
### Week 5, Lecture 9 and 10, Week 6, Lecture 11 and 12
- Reference: Understanding Maching Learning, Shalev-Shwartz and Ben David, Chapters 12 and 14.
- Reference: [Boyd Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/), Chapters 2 and 3 and 9
- Class notes: [Lecture 9](09.29.Math462.L9.pdf) [Lecture 11](10.06.Math462.Lecture11.pdf) [Lecture 12](10.08.Lecture12.pdf)

## Lectures Part 2
### Week 3, Lecture 6, Sept 17 (F)
- Loss design for classification: zero-one loss and hinge loss.
### Week 4, Lecture 7, Sept 22 (W) and  Week 4, Lecture 8, Sept 24 (F)
- Read the notes [Working version of course notes Part 2](https://github.com/adam-oberman/adam-oberman.github.io/blob/main/Lectures/Math462_Lecture_Notes_Part2.pdf) and discuss in class. 

## Lectures Part 1
### Week 1, lecture 1, Sept 1 (W)
- Introduction: example problems and datasets, meet and greet
- Reference: Murphy Ch1
### Week 1, Lecture 2, Sept 3 (F)
- Set up the supervised learning problem for regression
### Week 2, Lecture 3, Sept 8 (W)
- Regression, other losses, compare losses
- Calculus to find the minimizer of the (ELM) problem
- Reference: Course notes
### Week 2, Lecture 4, Sept 10 (F)
- Review of calculus and vector calculus, chain rule, gradient
- Compute gradient of the (EL) expected loss function
- Quadratic regression case
- References: Course Notes, Section "Gradients and Minimizers"
### Week 3, Lecture 5, Sept 15 (W)
- Instead of a lecture we will have a problem session, to work on HW1
### 
- [Old version of notes](https://github.com/adam-oberman/adam-oberman.github.io/blob/main/Lectures/Math462_Lecture_Notes_Part_1.pdf) and [Old version of notes Letter format](https://github.com/adam-oberman/adam-oberman.github.io/blob/main/Lectures/Math462_Lecture_Notes_Part_1_Letter.pdf)


## Zoom recordings
- (No recording for lectures 1 and 2)
- [Lecture 3](https://mcgill.zoom.us/rec/share/VKdYKjgxXbdlP9_8l3xcSKz7E2A7Z_gwyOpYjbO1n9XQ-gSIO51ITa9Ug83cjejV.ZFHqMEOCdcJpXMx0?startTime=1631109875000)
- Lecture 4 and future lectures are only available in mycourses (due to McGill zoom recording policy). 

